{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recurrent Neural Networks basics\n",
    "\n",
    "**Objective**\n",
    "\n",
    "- Understanding RNN capabilities with toy problems;\n",
    "- Getting familiar with `torch` RNN tools.\n",
    "\n",
    "**Resources and references**\n",
    "- [Stanford cheat-sheet on RNNs](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#overview)\n",
    "- Andrej Karpathy, [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- J. Brownlee, [\"5 Examples of Simple Sequence Prediction Problems for LSTMs\"](https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/)\n",
    "- [Christopher Olah's blog about LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (all his blog posts are great by the way)\n",
    "- [A very nice experimentation to play with LSTM](https://distill.pub/2016/handwriting/) (to go further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **1. Working with RNNs in PyTorch**\n",
    "  - *Elman's RNN*\n",
    "  - *Running an RNN*\n",
    "  - *What's happening inside?*\n",
    "- **2. Task typology**\n",
    "    - *Overview*\n",
    "    - *2.1. Toy problem: Many-to-one*\n",
    "    - *2.2. Toy problem: One-to-many*\n",
    "- **3. Project: handwriting synthesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with RNNs\n",
    "\n",
    "RNNs are a little bit different from their classical feedfoward cousins (you know their most common flavor: multilayerd perceptrons.)\n",
    "\n",
    "In a feedforward networks, fome function $f_{w,b}$ is applied on inputs $x$ to compute output $y$:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./static/perceptron.png\" />\n",
    "</p>\n",
    "\n",
    "$$\n",
    "y = f_{w,b}(x) = act(\\sum_i w_i x_i + b)\n",
    "$$\n",
    "\n",
    "$w$ (and $b$) are parameters of the network, and represents the weights of the connections between the neurons and their inputs (and a constant bias parameter). $act$ is the activation function.\n",
    "\n",
    "RNN are just slightly different: they take as input not only $x$, the current input, but also some of their previous computations $h$. In the simplest case, $h$ is equivalent to the output $y$ of our perceptron. We call it **the internal or hidden state**.\n",
    "\n",
    "$$\n",
    "y_t = h_t = act(\\sum_i w^{in}_i x_{t, i} + \\sum_j w^h h_{t-1, j} + b)\n",
    "$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./static/rnn.png\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "Note how we added some indices $t$ everywhere: now **time matters**. Your data points $x$ are no longer independent from each other, they form **sequences**, as we have seen in previous chapter.\n",
    "\n",
    "As simple RNNs are really just perceptrons with a supplementary input playing the role of a memory tape, we can also represent an RNN as a repeated application of a perceptron with two different inputs ($x_t$ and $h_{t-1}$):\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./static/rnn-unfold.png\" />\n",
    "</p>\n",
    "\n",
    "We will call this kind of representation **unfolded representations**, as recurrence and time are projected on the horizontal axis.\n",
    "\n",
    "The perceptron is here first applied at time $t$ on $x_t$ and $h_{t-1}$, its previous internal state. We have defined two different sets of weights $w_{in}$ and $w_h$ to parameterize the neuron, but this does not change much to the equation. Once the new state $h_{t}$ is computed, we move on to the next input $x_{t+1}$, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elman's RNN\n",
    "\n",
    "The neuron we have just defined is the building block of **Elman networks**, (from Jeffrey Locke Elman, their inventor).\n",
    "Elman's network, also called \"simple RNNs\" or just \"RNNs\", usually define an additional set of output weights to compute $y_t$ from $h_t$. This is like adding a layer of connections on top the internal state $h_t$ to perform the task we want (prediction, classification...)\n",
    "\n",
    "Simple RNNs are available in Pytorch through the [`RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) class. They do not implement the any output layer of weights. It will be up to you to add some more computations on top of some $h_t$ to perform your task. For now, let's look at how those neural networks work wihtout performing any task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 100-neurons RNN dealing with one dimensional inputs\n",
    "rnn = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wh weights shape:\", rnn.weight_hh_l0.shape)\n",
    "print(\"Win weights shape:\", rnn.weight_ih_l0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running an RNN\n",
    "\n",
    "Let's try running the RNN on some dummy data, here two succesive sine waves with different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave1 = np.sin(np.linspace(0, 4*np.pi, 100, endpoint=False))  # just some sinusoidal waves\n",
    "wave2 = np.sin(np.linspace(4*np.pi, 8*np.pi, 100)*5)\n",
    "\n",
    "wave = np.concatenate([wave1, wave2])\n",
    "\n",
    "plt.plot(wave)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always convert arrays to torch.Tensor!\n",
    "x = torch.Tensor(wave)\n",
    "\n",
    "# RNNs expect inputs of shape (sequence length, input dimension) (or (batch_size, sequence_length, input_dimension))\n",
    "# so here we need to reshape x from (100,) to (100, 1) (or (1, 100, 1))\n",
    "x =  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # deactivate automatic gradient computation, we don't need it here\n",
    "    # Apply the RNN on the wave. What are the outputs representing?\n",
    "    all_outputs, last_output = ...\n",
    "\n",
    "print(all_outputs.size(), \"\\n\", last_output.size())\n",
    "print(all_outputs, \"\\n\", last_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's happening inside?\n",
    "\n",
    "Our 100-dimensional vectors represent the hidden states of our 100 neurons, for all input data steps.\n",
    "Let's have a look at those hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the activity of some random neurons inside the RNN\n",
    "...\n",
    "# The activity of all neurons inside the RNN at a timestep t is called the \"hidden_state\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task typology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview: What do we do with it ?\n",
    "\n",
    "There are many ways of using RNNs in a neural network architecture, as they allow to fold and unfold time in your data.\n",
    "\n",
    "\n",
    "For instance, you can imagine generating long captions for single images, going from a static component (an image) to a sequence of words of any length. On the contrary, a sequence of words could be compressed into a single label representing overall sentiment of a sentence (\"happy\" or \"sad\"). Or, you can use RNNs to translate a French sentence of size $n$ into an English sentence of size $m$. \n",
    "\n",
    "As you can see, RNNs allow to play along the time axis in various manners, depending on the task at hand, which make them incredibly more powerful than feedforwad neural networks in many situations.\n",
    "\n",
    "In the following figures, green vectors $x$ represent input sequence or vector, green vector $a$ represents hidden states, and red vectors $y$ represent output of the neural network, which can be equal to $a$ or post-processed by some other neural network, like a layer of neurons trained to output class probabilities. RNNs are represented unfolded in time to make sequences apparent.\n",
    "\n",
    "These variations of RNN behavior fall into four main classes of networks.\n",
    "\n",
    "- **Many-to-many models**: from a sequence/timeseries, output another sequence/timeseries of **same length.** Ex: online speech recognition.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./static/rnn-many-to-many-same-ltr.png\" width=500/>\n",
    "</p>\n",
    "\n",
    "- **Many-to-one, or seq-to-vec models**: from a sequence, output a single vector. Ex: sequence classification tasks, like sentence sentiment analysis.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./static/rnn-many-to-one-ltr.png\" width=500/>\n",
    "</p>\n",
    "\n",
    "- **One-to-many, or vec-to-seq models**: from a single vector, output a sequence. Ex: image captionning or music generation.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./static/rnn-one-to-many-ltr.png\" width=500/>\n",
    "</p>\n",
    "\n",
    "- **Encoder-decoder models**: from a sequence/timeseries, output another sequence/timeseries of **different length.** Ex: machine translation.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./static/rnn-many-to-many-different-ltr.png\" width=500/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-one: the pocket calculator network\n",
    "\n",
    "Let's design a neural network able to perform a **simple classfication task**: *tell if an arithmetical expression will give negative or positive results.*\n",
    "\n",
    "Inputs are sequences of numbers as follow:\n",
    "\n",
    "- every step of input is made of two number: an operand and an operator.\n",
    "- the first number (the operand) is the number on which we will perform the operation\n",
    "- the second number (the operator) is either 1 or 0. 1 indicates that we want to add that number to the total, 0 indicates that we want to substract that number.\n",
    "\n",
    "Hence, the following expression:\n",
    "$$+ 1 + 2 - 5 + 4 - 6$$\n",
    "\n",
    "is represented by the input sequence:\n",
    "\n",
    "```\n",
    "[[1, 1], \n",
    " [2, 1], \n",
    " [5, 0], \n",
    " [4, 1],\n",
    " [6, 0]]\n",
    "```\n",
    "\n",
    "The target class is the expected sign of the total (here  -4, so **negative class**)\n",
    "\n",
    "All sequences are 5 operands long. All operands are single digits (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_operand_sequences(n_sequences, seq_length=5):\n",
    "    digits = list(range(0, 10))\n",
    "    operators = [0, 1]\n",
    "    op_symbols = [\"-\", \"+\"]\n",
    "    x = np.zeros((n_sequences, seq_length, 2))\n",
    "    y = np.zeros(n_sequences)\n",
    "    for i in range(n_sequences):\n",
    "        operands = np.random.choice(digits, seq_length, replace=True)\n",
    "        ops = np.random.choice(operators, seq_length, replace=True)\n",
    "        seq = np.vstack([operands, ops])\n",
    "        x[i] = seq.T\n",
    "        expr = \"\"\n",
    "        for digit, op in zip(operands, ops):\n",
    "            expr += op_symbols[operators.index(op)] + str(digit)\n",
    "        total = eval(expr)\n",
    "        y[i] = 1 if total > 0 else 0\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_operand_sequences(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calculator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Remember you can store and do everything \n",
    "        # you need in nn.Module objects\n",
    "        # by defining your own methods and attributes.\n",
    "        # Pytorch is very permissive!\n",
    "        \n",
    "        # Number of neurons in the RNN\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create an RNN for the task\n",
    "        # and do not forget batch_first=True\n",
    "        # and an output classification layer\n",
    "        self.rnn = ...\n",
    "        self.fc = ...\n",
    "\n",
    "        # Note that we won't add an activation function like softmax to the last layer\n",
    "        # for now. Pytorch prefers training classifier using unormalized\n",
    "        # outputs when choosing cross-entropy loss (see documentation).\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform forward (prediction) pass.\n",
    "        # Get only the last state on a sequence and pass it to the classification layer.\n",
    "        # This state will represent the accumulated information\n",
    "        # along the sequence of inputs.\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        output = ...\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"  # change it to \"cuda\" if you can run on GPU\n",
    "\n",
    "network = Calculator(hidden_size=100).to(device)\n",
    "\n",
    "print(network)\n",
    "\n",
    "#### Training loop ####\n",
    "num_epochs = 50  # change it as you wish\n",
    "\n",
    "network.train()  # training mode: on\n",
    "\n",
    "# Loss: cross entropy (this is a classfication task!)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rule (optimizer). We will go with a refinement\n",
    "# of the gradient descent, called Adam.\n",
    "opt = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Convert everything to torch.Tensor and create a Dataset and \n",
    "# a DataLoader objects.\n",
    "# They are used to hold your data during training, and help\n",
    "# you define batch size or data transformations for instance.\n",
    "# For now, we need only a simple TensorDataset (taking Tensors as \n",
    "# inputs) and a DataLoader with our chosen batch size.\n",
    "batch_size = 10  # change it as you wish\n",
    "\n",
    "x_train = torch.Tensor(x).to(device)\n",
    "y_train = torch.Tensor(y).to(torch.long).to(device)  # class labels must be converted to torch.long\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "losses = []\n",
    "for e in range(num_epochs):\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    for x_i, y_i in dataloader:  # for each batch of data\n",
    "        \n",
    "        # training loop !\n",
    "        ...\n",
    "        \n",
    "        loss = ...\n",
    "\n",
    "        # Save loss values somewhere to plot the learning curve\n",
    "        loss = loss.detach().numpy()\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "        print(f\"Epoch {e} - Loss {np.mean(epoch_loss):.3f}\", end=\"\\r\")\n",
    "\n",
    "    losses.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = make_operand_sequences(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test network\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = network(torch.Tensor(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", np.sum(np.argmax(y_pred.numpy(), axis=1) == y_test) / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-many: the oscillator\n",
    "\n",
    "One-to-many networks require a bit of mental exercise to set up a task properly. In the following example, we will try to solve a rather simple problem: can we train an RNN to generate a sinusoidal wave (many outputs) given a frequency as (single) input?\n",
    "\n",
    "To generate a sequence from a vector, an RNN needs two conditions (which will recall the basic recurrence reasoning in maths; hence the name of RNN):\n",
    "- a good initial condition;\n",
    "- a recurrence relationship between RNN's outputs.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./static/rnn-one-to-many-ltr.png\" width=500/>\n",
    "</p>\n",
    "\n",
    "The initial condition here is the green $x$ in the figure, or the \"one\" in \"one-to-many\". It is here to drive the network in an initial state from which it can autonomously generate the whole sequence. In our case, this could be a simple one-hot vector representing the frequency of the sinusoidal wave we want to generate. \n",
    "\n",
    "This $x$ can be fed to the network as a first input before the network receives its own previous outputs as input. This is exactly what can be seen in the schema. This is called a pre-inject.\n",
    "\n",
    "Other techniques exist. $x$ can also be used directly as a first hidden state for the RNN. This is called an init-inject. Or $x$ can be added to the network outputs at every timestep, like a constant reminder of the initial goal. This is called a par-inject.\n",
    "\n",
    "Once the initialization is done, the network will then work on its own outputs to generate the whole sequence, by recurrence.\n",
    "\n",
    "In this example, we will do an init-inject initialization. We will project the 3-d vector holding frequency information into hidden state space, and use the result to initialize the first hidden state of the network (which is initialize to 0 by default). This can be done by adding a layer of connections between the frequency vector and the hidden space, that will \"reshape\" the frequency vector to be as big as the hidden state vector. Once reshaped, the transformed frequency can be directly fed as hidden state input to the RNN.\n",
    "\n",
    "From there, the network will have to predict a first point of the wave. This point will serve as next input to the network, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waves(n_sequences, frequencies, length):\n",
    "    x = np.zeros((n_sequences, 1, len(frequencies)))\n",
    "    y = np.zeros((n_sequences, length, 1))\n",
    "    for i in range(n_sequences):\n",
    "        fs = np.random.choice(frequencies)\n",
    "        waves = np.sin(2*np.pi*fs*np.linspace(0, 1, length))\n",
    "        fs_idx = frequencies.index(fs)\n",
    "        x[i, 0, fs_idx] = 1  # one hot encode frequencies: 0 for all, 1 for the current frequency\n",
    "        y[i, :, 0] = waves\n",
    "    \n",
    "    # return shape (n, length, 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = [1, 5, 10]  # frequency values\n",
    "length = 50  # length of the sinusoidal wave\n",
    "\n",
    "x, y = make_waves(1000, frequencies, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape: [n, length, dimension]\n",
    "# Note that length is 1 here, as we only give a single input to the network.\n",
    "# One x per wave, representing the frequency class\n",
    "print(x.shape, \"\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineWaveGenerator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, n_frequencies, length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.length = length\n",
    "\n",
    "        # Create a layer to transform frequency into hidden states\n",
    "        self.input_to_hidden = ...\n",
    "        # Create an RNN\n",
    "        self.rnn = ...\n",
    "        # Create an output layer to transform hidden states into a 1-d point of sinusoidal wave\n",
    "        self.hidden_to_output = ...\n",
    "\n",
    "    def forward(self, freq):\n",
    "                \n",
    "        # store outputs\n",
    "        outputs = torch.zeros(x.size(0), self.length, 1).to(freq.device)\n",
    "        \n",
    "        # Mind the shapes here:\n",
    "        # RNN outputs do not support batch_first=True... \n",
    "        # You may need to transpose things to keep everything coherent.\n",
    "\n",
    "        # init-inject\n",
    "        # You should go from input vector of shape [n, 1, frequencies]\n",
    "        # to init hidden state of shape [1, n, hidden_size]\n",
    "        \n",
    "        hidden = ...\n",
    "        \n",
    "        # first input is null\n",
    "        # zeros: [n, length=1, dim]\n",
    "        x = ...\n",
    "        \n",
    "        # Generate the wave by recurrence\n",
    "        for i in range(0, self.length):\n",
    "                \n",
    "            y = ...\n",
    "            \n",
    "            # store outputs\n",
    "            outputs[:, i, 0] = y.squeeze()\n",
    "            \n",
    "            # Use output as next input\n",
    "            x = y\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SineWaveGenerator(hidden_size=100, n_frequencies=3, length=length)\n",
    "\n",
    "print(network)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Ok now do it yourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = make_waves(100, frequencies, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = network(torch.Tensor(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE:\", np.mean((y_test - y_pred.numpy())**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
